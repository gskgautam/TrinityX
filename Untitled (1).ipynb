{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "098720a7-3ec3-43ea-ab86-88ea14e83550",
   "metadata": {},
   "source": [
    "Alpaca Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6df49fd6-0ef8-4bef-8ebe-b5535fea3f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fdc5a33-ac2a-479b-83e2-55f36d5976ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.50.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.30.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "951c7866-852f-425a-9914-1a5161c315ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.1.0+cu118)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.50.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.6.0)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.5.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.30.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.25.0->peft) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.25.0->peft) (2024.12.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.25.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.25.0->peft) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.1.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.21.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "399ae87d-cafe-45d1-825f-4adc19f2bd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.24.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.16)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.30.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2022.12.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88b887be-f7c2-4ab2-b131-6f32d52e63c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.30.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2022.12.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install huggingface_hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "370a4272-2796-4f8a-b2b8-6deaab0c2765",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc71ae31-6519-41c1-9714-46744de5d9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0) (6.0.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0) (2.1.0+cu118)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0) (0.30.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0) (0.5.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.12.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.26.0) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.26.0) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->accelerate>=0.26.0) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->accelerate>=0.26.0) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install 'accelerate>=0.26.0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63355dbd-948e-4761-bb75-c7d43f415952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.50.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.30.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.67.1)\n",
      "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu118)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.6.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->transformers[torch]) (5.9.6)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers[torch]) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers[torch]) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->transformers[torch]) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->transformers[torch]) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->transformers[torch]) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->transformers[torch]) (2.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0->transformers[torch]) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0->transformers[torch]) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers[torch]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7b02b38-9986-46c2-92d1-2a89b30937c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73d0fb1e-2436-4efe-8f26-2d26a2f4e2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "086cb195-2504-44cd-9ca9-3612300491f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58cfd8468e224dd8abad26f5378057a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Base model loaded.\n",
      "âœ… Dataset loaded successfully.\n",
      "âœ… Dataset loaded.\n",
      "âœ… Data tokenized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "No label_names provided for model class `PeftModel`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 33554432 / 6771970048 (0.50%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26001' max='26001' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26001/26001 6:30:06, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.067500</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Checking if adapter layers have been updated...\n",
      "ğŸ” Trainable: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight, Norm: 19.69855499267578\n",
      "ğŸ” Trainable: base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight, Norm: 20.208498001098633\n",
      "ğŸ” Trainable: base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight, Norm: 16.079959869384766\n",
      "ğŸ” Trainable: base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight, Norm: 14.179100036621094\n",
      "ğŸ” Trainable: base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight, Norm: 19.536806106567383\n",
      "ğŸ” Trainable: base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight, Norm: 16.97258949279785\n",
      "ğŸ” Trainable: base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight, Norm: 14.844531059265137\n",
      "ğŸ” Trainable: base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight, Norm: 15.475481033325195\n",
      "ğŸ” Trainable: base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight, Norm: 20.135902404785156\n",
      "ğŸ” Trainable: base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight, Norm: 16.639862060546875\n",
      "ğŸ” Trainable: base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight, Norm: 17.236536026000977\n",
      "ğŸ” Trainable: base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight, Norm: 17.31879234313965\n",
      "ğŸ” Trainable: base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight, Norm: 20.805450439453125\n",
      "ğŸ” Trainable: base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight, Norm: 18.393022537231445\n",
      "ğŸ” Trainable: base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight, Norm: 17.498403549194336\n",
      "ğŸ” Trainable: base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight, Norm: 17.302488327026367\n",
      "ğŸ” Trainable: base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight, Norm: 20.974193572998047\n",
      "ğŸ” Trainable: base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight, Norm: 18.554866790771484\n",
      "ğŸ” Trainable: base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight, Norm: 17.962419509887695\n",
      "ğŸ” Trainable: base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight, Norm: 17.51983070373535\n",
      "ğŸ” Trainable: base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight, Norm: 20.894012451171875\n",
      "ğŸ” Trainable: base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight, Norm: 18.103435516357422\n",
      "ğŸ” Trainable: base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight, Norm: 17.631393432617188\n",
      "ğŸ” Trainable: base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight, Norm: 17.008684158325195\n",
      "ğŸ” Trainable: base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight, Norm: 20.488435745239258\n",
      "ğŸ” Trainable: base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight, Norm: 18.64004135131836\n",
      "ğŸ” Trainable: base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight, Norm: 17.655385971069336\n",
      "ğŸ” Trainable: base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight, Norm: 17.348337173461914\n",
      "ğŸ” Trainable: base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight, Norm: 20.614070892333984\n",
      "ğŸ” Trainable: base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight, Norm: 18.7622127532959\n",
      "ğŸ” Trainable: base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight, Norm: 17.445409774780273\n",
      "ğŸ” Trainable: base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight, Norm: 17.355411529541016\n",
      "ğŸ” Trainable: base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight, Norm: 20.5278377532959\n",
      "ğŸ” Trainable: base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight, Norm: 18.72806167602539\n",
      "ğŸ” Trainable: base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight, Norm: 17.419790267944336\n",
      "ğŸ” Trainable: base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight, Norm: 17.24418830871582\n",
      "ğŸ” Trainable: base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight, Norm: 20.412206649780273\n",
      "ğŸ” Trainable: base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight, Norm: 19.126588821411133\n",
      "ğŸ” Trainable: base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight, Norm: 17.161073684692383\n",
      "ğŸ” Trainable: base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight, Norm: 17.233585357666016\n",
      "ğŸ” Trainable: base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight, Norm: 20.49327850341797\n",
      "ğŸ” Trainable: base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight, Norm: 19.120746612548828\n",
      "ğŸ” Trainable: base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight, Norm: 17.405101776123047\n",
      "ğŸ” Trainable: base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight, Norm: 17.269001007080078\n",
      "ğŸ” Trainable: base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight, Norm: 20.102130889892578\n",
      "ğŸ” Trainable: base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight, Norm: 18.823556900024414\n",
      "ğŸ” Trainable: base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight, Norm: 16.829483032226562\n",
      "ğŸ” Trainable: base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight, Norm: 16.790523529052734\n",
      "ğŸ” Trainable: base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight, Norm: 19.867952346801758\n",
      "ğŸ” Trainable: base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight, Norm: 18.91082191467285\n",
      "ğŸ” Trainable: base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight, Norm: 16.938331604003906\n",
      "ğŸ” Trainable: base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight, Norm: 16.91809844970703\n",
      "ğŸ” Trainable: base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight, Norm: 19.820632934570312\n",
      "ğŸ” Trainable: base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight, Norm: 19.050525665283203\n",
      "ğŸ” Trainable: base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight, Norm: 16.882404327392578\n",
      "ğŸ” Trainable: base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight, Norm: 16.95608139038086\n",
      "ğŸ” Trainable: base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight, Norm: 19.589441299438477\n",
      "ğŸ” Trainable: base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight, Norm: 19.008710861206055\n",
      "ğŸ” Trainable: base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight, Norm: 16.83184242248535\n",
      "ğŸ” Trainable: base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight, Norm: 16.802595138549805\n",
      "ğŸ” Trainable: base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight, Norm: 19.83679962158203\n",
      "ğŸ” Trainable: base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight, Norm: 18.94509506225586\n",
      "ğŸ” Trainable: base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight, Norm: 17.196474075317383\n",
      "ğŸ” Trainable: base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight, Norm: 17.161169052124023\n",
      "ğŸ” Trainable: base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight, Norm: 19.89643669128418\n",
      "ğŸ” Trainable: base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight, Norm: 19.130615234375\n",
      "ğŸ” Trainable: base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight, Norm: 16.564071655273438\n",
      "ğŸ” Trainable: base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight, Norm: 16.755863189697266\n",
      "ğŸ” Trainable: base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight, Norm: 19.872133255004883\n",
      "ğŸ” Trainable: base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight, Norm: 19.310117721557617\n",
      "ğŸ” Trainable: base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight, Norm: 17.1380615234375\n",
      "ğŸ” Trainable: base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight, Norm: 17.372102737426758\n",
      "ğŸ” Trainable: base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight, Norm: 19.62700653076172\n",
      "ğŸ” Trainable: base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight, Norm: 19.24454689025879\n",
      "ğŸ” Trainable: base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight, Norm: 17.811861038208008\n",
      "ğŸ” Trainable: base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight, Norm: 18.039527893066406\n",
      "ğŸ” Trainable: base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight, Norm: 20.18467903137207\n",
      "ğŸ” Trainable: base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight, Norm: 19.400671005249023\n",
      "ğŸ” Trainable: base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight, Norm: 17.985414505004883\n",
      "ğŸ” Trainable: base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight, Norm: 17.9179744720459\n",
      "ğŸ” Trainable: base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight, Norm: 20.099172592163086\n",
      "ğŸ” Trainable: base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight, Norm: 19.185392379760742\n",
      "ğŸ” Trainable: base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight, Norm: 18.175241470336914\n",
      "ğŸ” Trainable: base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight, Norm: 18.231149673461914\n",
      "ğŸ” Trainable: base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight, Norm: 20.478004455566406\n",
      "ğŸ” Trainable: base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight, Norm: 19.273880004882812\n",
      "ğŸ” Trainable: base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight, Norm: 17.774831771850586\n",
      "ğŸ” Trainable: base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight, Norm: 17.497657775878906\n",
      "ğŸ” Trainable: base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight, Norm: 19.851669311523438\n",
      "ğŸ” Trainable: base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight, Norm: 19.081708908081055\n",
      "ğŸ” Trainable: base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight, Norm: 18.178619384765625\n",
      "ğŸ” Trainable: base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight, Norm: 18.111116409301758\n",
      "ğŸ” Trainable: base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight, Norm: 20.44747543334961\n",
      "ğŸ” Trainable: base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight, Norm: 19.435400009155273\n",
      "ğŸ” Trainable: base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight, Norm: 18.1256046295166\n",
      "ğŸ” Trainable: base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight, Norm: 17.723445892333984\n",
      "ğŸ” Trainable: base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight, Norm: 20.574922561645508\n",
      "ğŸ” Trainable: base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight, Norm: 19.33599853515625\n",
      "ğŸ” Trainable: base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight, Norm: 18.36996078491211\n",
      "ğŸ” Trainable: base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight, Norm: 18.108701705932617\n",
      "ğŸ” Trainable: base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight, Norm: 20.524593353271484\n",
      "ğŸ” Trainable: base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight, Norm: 19.121641159057617\n",
      "ğŸ” Trainable: base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight, Norm: 17.36888313293457\n",
      "ğŸ” Trainable: base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight, Norm: 16.76012420654297\n",
      "ğŸ” Trainable: base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight, Norm: 20.487918853759766\n",
      "ğŸ” Trainable: base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight, Norm: 19.249208450317383\n",
      "ğŸ” Trainable: base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight, Norm: 18.09611701965332\n",
      "ğŸ” Trainable: base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight, Norm: 17.862394332885742\n",
      "ğŸ” Trainable: base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight, Norm: 20.69956398010254\n",
      "ğŸ” Trainable: base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight, Norm: 19.845401763916016\n",
      "ğŸ” Trainable: base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight, Norm: 18.594839096069336\n",
      "ğŸ” Trainable: base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight, Norm: 17.840829849243164\n",
      "ğŸ” Trainable: base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight, Norm: 20.482982635498047\n",
      "ğŸ” Trainable: base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight, Norm: 19.388179779052734\n",
      "ğŸ” Trainable: base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight, Norm: 18.206859588623047\n",
      "ğŸ” Trainable: base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight, Norm: 17.7409725189209\n",
      "ğŸ” Trainable: base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight, Norm: 20.93814468383789\n",
      "ğŸ” Trainable: base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight, Norm: 19.585346221923828\n",
      "ğŸ” Trainable: base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight, Norm: 17.700542449951172\n",
      "ğŸ” Trainable: base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight, Norm: 17.24711799621582\n",
      "ğŸ” Trainable: base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight, Norm: 19.97378158569336\n",
      "ğŸ” Trainable: base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight, Norm: 19.590137481689453\n",
      "ğŸ” Trainable: base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight, Norm: 16.602642059326172\n",
      "ğŸ” Trainable: base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight, Norm: 17.408203125\n",
      "ğŸ” Trainable: base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight, Norm: 18.651079177856445\n",
      "ğŸ” Trainable: base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight, Norm: 19.556087493896484\n",
      "ğŸ” Trainable: base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight, Norm: 15.907849311828613\n",
      "ğŸ” Trainable: base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight, Norm: 17.34032440185547\n",
      "âœ… Adapter model trained.\n",
      "ğŸ” Debug: Checking parameter differences between base and adapter models...\n",
      "ğŸ›‘ Skipping 'base_model.model.model.embed_tokens.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.0.self_attn.k_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.0.self_attn.o_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.0.mlp.gate_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.0.mlp.up_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.0.mlp.down_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.0.input_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.0.post_attention_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.1.self_attn.k_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.1.self_attn.o_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.1.mlp.gate_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.1.mlp.up_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.1.mlp.down_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.1.input_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.1.post_attention_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.2.self_attn.k_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.2.self_attn.o_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.2.mlp.gate_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.2.mlp.up_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.2.mlp.down_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.2.input_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.2.post_attention_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.3.self_attn.k_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.3.self_attn.o_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.3.mlp.gate_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.3.mlp.up_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.3.mlp.down_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.3.input_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.3.post_attention_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.4.self_attn.k_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.4.self_attn.o_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.4.mlp.gate_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.4.mlp.up_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.4.mlp.down_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.4.input_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.4.post_attention_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.5.self_attn.k_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.5.self_attn.o_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.5.mlp.gate_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.5.mlp.up_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.5.mlp.down_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.5.input_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.5.post_attention_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.6.self_attn.k_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.6.self_attn.o_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.6.mlp.gate_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.6.mlp.up_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.6.mlp.down_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.6.input_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.6.post_attention_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.7.self_attn.k_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.7.self_attn.o_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.7.mlp.gate_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.7.mlp.up_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.7.mlp.down_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.7.input_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.7.post_attention_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.8.self_attn.k_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.8.self_attn.o_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.8.mlp.gate_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.8.mlp.up_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.8.mlp.down_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.8.input_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.8.post_attention_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.9.self_attn.k_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.9.self_attn.o_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.9.mlp.gate_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.9.mlp.up_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.9.mlp.down_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.9.input_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.9.post_attention_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.10.self_attn.k_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.10.self_attn.o_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.10.mlp.gate_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.10.mlp.up_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.10.mlp.down_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.10.input_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.10.post_attention_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.11.self_attn.k_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.11.self_attn.o_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.11.mlp.gate_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.11.mlp.up_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.11.mlp.down_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.11.input_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.11.post_attention_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.12.self_attn.k_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.12.self_attn.o_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.12.mlp.gate_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.12.mlp.up_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.12.mlp.down_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.12.input_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.12.post_attention_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.13.self_attn.k_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.13.self_attn.o_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.13.mlp.gate_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.13.mlp.up_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.13.mlp.down_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.13.input_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.13.post_attention_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.14.self_attn.k_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.14.self_attn.o_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.14.mlp.gate_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.14.mlp.up_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.14.mlp.down_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.14.input_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.14.post_attention_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.15.self_attn.k_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.15.self_attn.o_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.15.mlp.gate_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.15.mlp.up_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.15.mlp.down_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.15.input_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.15.post_attention_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.16.self_attn.k_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.16.self_attn.o_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.16.mlp.gate_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.16.mlp.up_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.16.mlp.down_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.16.input_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.16.post_attention_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.17.self_attn.k_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.17.self_attn.o_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.17.mlp.gate_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.17.mlp.up_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.17.mlp.down_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.17.input_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.17.post_attention_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.18.self_attn.k_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.18.self_attn.o_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.18.mlp.gate_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.18.mlp.up_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.18.mlp.down_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.18.input_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.18.post_attention_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.19.self_attn.k_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.19.self_attn.o_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.19.mlp.gate_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.19.mlp.up_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.19.mlp.down_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.19.input_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.19.post_attention_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.20.self_attn.k_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.20.self_attn.o_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.20.mlp.gate_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.20.mlp.up_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.20.mlp.down_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.20.input_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.20.post_attention_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.21.self_attn.k_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.21.self_attn.o_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.21.mlp.gate_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.21.mlp.up_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.21.mlp.down_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.21.input_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.21.post_attention_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.22.self_attn.k_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.22.self_attn.o_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.22.mlp.gate_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.22.mlp.up_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.22.mlp.down_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.22.input_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.22.post_attention_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.23.self_attn.k_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.23.self_attn.o_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.23.mlp.gate_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.23.mlp.up_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.23.mlp.down_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.23.input_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.23.post_attention_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.24.self_attn.k_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.24.self_attn.o_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.24.mlp.gate_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.24.mlp.up_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.24.mlp.down_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.24.input_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.24.post_attention_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.25.self_attn.k_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.25.self_attn.o_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.25.mlp.gate_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.25.mlp.up_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.25.mlp.down_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.25.input_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.25.post_attention_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.26.self_attn.k_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.26.self_attn.o_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.26.mlp.gate_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.26.mlp.up_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.26.mlp.down_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.26.input_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.26.post_attention_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.27.self_attn.k_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.27.self_attn.o_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.27.mlp.gate_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.27.mlp.up_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.27.mlp.down_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.27.input_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.27.post_attention_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.28.self_attn.k_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.28.self_attn.o_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.28.mlp.gate_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.28.mlp.up_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.28.mlp.down_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.28.input_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.28.post_attention_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.29.self_attn.k_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.29.self_attn.o_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.29.mlp.gate_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.29.mlp.up_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.29.mlp.down_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.29.input_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.29.post_attention_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.30.self_attn.k_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.30.self_attn.o_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.30.mlp.gate_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.30.mlp.up_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.30.mlp.down_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.30.input_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.30.post_attention_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.31.self_attn.k_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.31.self_attn.o_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.31.mlp.gate_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.31.mlp.up_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.31.mlp.down_proj.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.31.input_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.layers.31.post_attention_layernorm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.model.norm.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸ›‘ Skipping 'base_model.model.lm_head.weight': Not in base model (Possibly a LoRA-specific layer).\n",
      "ğŸš¨ Task vector is empty! Ensure adapter model was trained correctly.\n",
      "ğŸ” Debugging steps:\n",
      "1. Verify that training loss decreased during training.\n",
      "2. Check that LoRA layers are updating using print_trainable_parameters().\n",
      "3. Inspect saved adapter weights to confirm changes.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "ğŸš¨ Task vector is empty. Ensure adapter model was trained correctly.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 173\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m final_model\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m: \n\u001b[0;32m--> 173\u001b[0m     final_model \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ‰ Model training and adaptation completed successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 163\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    160\u001b[0m adapter_model \u001b[38;5;241m=\u001b[39m train_lora_adapter(base_model, tokenized_train_data, tokenized_test_data, adapter_name, device)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Adapter model trained.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 163\u001b[0m task_vector \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_task_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Compute task vector\u001b[39;00m\n\u001b[1;32m    164\u001b[0m gamma_value \u001b[38;5;241m=\u001b[39m compute_gamma_value(task_vector)  \u001b[38;5;66;03m# Compute gamma value\u001b[39;00m\n\u001b[1;32m    166\u001b[0m final_model \u001b[38;5;241m=\u001b[39m apply_task_vector(base_model, task_vector, gamma_value, device)  \u001b[38;5;66;03m# Apply task vector to base model\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 115\u001b[0m, in \u001b[0;36mcompute_task_vector\u001b[0;34m(base_model, adapter_model, device)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2. Check that LoRA layers are updating using print_trainable_parameters().\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3. Inspect saved adapter weights to confirm changes.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸš¨ Task vector is empty. Ensure adapter model was trained correctly.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Computed task vector with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnonzero_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m nonzero layers.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    118\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(task_vector, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_vector.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Save task vector to disk\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: ğŸš¨ Task vector is empty. Ensure adapter model was trained correctly."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import Dataset\n",
    "\n",
    "def load_dataset():\n",
    "    \"\"\"Load the Alpaca dataset.\"\"\"\n",
    "    try:\n",
    "        alpaca_train = pd.read_json(\"/workspace/Dataset/Alpaca/Alpaca_Train.json\")\n",
    "        alpaca_test = pd.read_json(\"/workspace/Dataset/Alpaca/Alpaca_Test.json\")\n",
    "        print(\"âœ… Dataset loaded successfully.\")\n",
    "        return alpaca_train, alpaca_test\n",
    "    except Exception as e:\n",
    "        print(f\"ğŸš¨ Error loading dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "def tokenize_data(tokenizer, dataset, is_train=True, device='cpu'):\n",
    "    \"\"\"Tokenize input and add labels as the same input for supervised learning.\"\"\"\n",
    "    column1, column2, label_column = ('instruction', 'input', 'output') if is_train else ('instruction', 'output', 'output')\n",
    "\n",
    "    tokenized = dataset.apply(lambda row: tokenizer(f\"{row[column1]} {row[column2]}\", truncation=True, padding='max_length', max_length=512), axis=1)\n",
    "    \n",
    "    tokenized_dict = {\n",
    "        \"input_ids\": [x['input_ids'] for x in tokenized],\n",
    "        \"attention_mask\": [x['attention_mask'] for x in tokenized],\n",
    "        \"labels\": [x['input_ids'] for x in tokenized]  # Labels are the same as input_ids for CausalLM\n",
    "    }\n",
    "    \n",
    "    # Move the data to GPU or CPU depending on the device\n",
    "    tokenized_dict[\"input_ids\"] = [torch.tensor(x).to(device) for x in tokenized_dict[\"input_ids\"]]\n",
    "    tokenized_dict[\"attention_mask\"] = [torch.tensor(x).to(device) for x in tokenized_dict[\"attention_mask\"]]\n",
    "    tokenized_dict[\"labels\"] = [torch.tensor(x).to(device) for x in tokenized_dict[\"labels\"]]\n",
    "\n",
    "    return Dataset.from_dict(tokenized_dict)\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"Print trainable parameters in the model.\"\"\"\n",
    "    trainable_params = 0\n",
    "    all_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"Trainable parameters: {trainable_params} / {all_params} ({100 * trainable_params / all_params:.2f}%)\")\n",
    "\n",
    "def train_lora_adapter(base_model, train_data, eval_data, adapter_name, device):\n",
    "    \"\"\"Train a LoRA adapter on the base model with debugging logs.\"\"\"\n",
    "    lora_config = LoraConfig(r=64, lora_alpha=32, lora_dropout=0.1)\n",
    "    model = get_peft_model(base_model, lora_config).to(device)\n",
    "\n",
    "    print_trainable_parameters(model)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results/{adapter_name}\",\n",
    "        per_device_train_batch_size=2,\n",
    "        num_train_epochs=1,  \n",
    "        learning_rate=5e-4,  # Explicit learning rate\n",
    "        logging_dir=f\"./logs/{adapter_name}\",\n",
    "        save_steps=500,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        logging_steps=10,\n",
    "        save_total_limit=2\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=eval_data,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    print(\"âœ… Checking if adapter layers have been updated...\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"ğŸ” Trainable: {name}, Norm: {torch.norm(param).item()}\")\n",
    "\n",
    "    model.save_pretrained(f\"./results/{adapter_name}\")\n",
    "    return model\n",
    "\n",
    "def compute_task_vector(base_model, adapter_model, device):\n",
    "    \"\"\"Compute the task vector for a given adapter model with detailed debugging.\"\"\"\n",
    "    task_vector = {}\n",
    "\n",
    "    base_state_dict = base_model.state_dict()\n",
    "    adapter_state_dict = adapter_model.state_dict()\n",
    "\n",
    "    print(\"ğŸ” Debug: Checking parameter differences between base and adapter models...\")\n",
    "\n",
    "    nonzero_count = 0  # Track number of parameters with actual differences\n",
    "\n",
    "    for k in adapter_state_dict.keys():\n",
    "        if k in base_state_dict:\n",
    "            diff = adapter_state_dict[k] - base_state_dict[k]\n",
    "            norm_diff = torch.norm(diff).item()\n",
    "\n",
    "            if norm_diff > 0:\n",
    "                task_vector[k] = diff\n",
    "                nonzero_count += 1\n",
    "                print(f\"âœ… Layer '{k}' has a nonzero difference (Norm: {norm_diff:.6f})\")\n",
    "            else:\n",
    "                print(f\"âš ï¸ Layer '{k}' has no difference (Norm: {norm_diff:.6f}) - Check if training updated parameters!\")\n",
    "        else:\n",
    "            print(f\"ğŸ›‘ Skipping '{k}': Not in base model (Possibly a LoRA-specific layer).\")\n",
    "\n",
    "    if not task_vector:\n",
    "        print(\"ğŸš¨ Task vector is empty! Ensure adapter model was trained correctly.\")\n",
    "        print(\"ğŸ” Debugging steps:\")\n",
    "        print(\"1. Verify that training loss decreased during training.\")\n",
    "        print(\"2. Check that LoRA layers are updating using print_trainable_parameters().\")\n",
    "        print(\"3. Inspect saved adapter weights to confirm changes.\")\n",
    "        raise ValueError(\"ğŸš¨ Task vector is empty. Ensure adapter model was trained correctly.\")\n",
    "\n",
    "    print(f\"âœ… Computed task vector with {nonzero_count} nonzero layers.\")\n",
    "    torch.save(task_vector, \"task_vector.pt\")  # Save task vector to disk\n",
    "    print(\"âœ… Task vector saved successfully.\")\n",
    "    return task_vector\n",
    "\n",
    "def compute_gamma_value(task_vector):\n",
    "    \"\"\"Compute a single gamma value based on the task vector norm.\"\"\"\n",
    "    norm = torch.norm(torch.cat([v.flatten() for v in task_vector.values()]))\n",
    "    gamma_value = 1.0  # Since only one adapter is used, it fully contributes to the final model.\n",
    "\n",
    "    with open(\"gamma_value.json\", \"w\") as f:\n",
    "        json.dump({\"gamma\": gamma_value}, f)\n",
    "    print(f\"âœ… Gamma value {gamma_value} saved successfully.\")\n",
    "\n",
    "    return gamma_value\n",
    "\n",
    "def apply_task_vector(base_model, task_vector, gamma_value, device):\n",
    "    \"\"\"Apply a weighted sum of the task vector to the base model.\"\"\"\n",
    "    updated_model_state = base_model.state_dict()\n",
    "    for k in task_vector.keys():\n",
    "        updated_model_state[k] += gamma_value * task_vector[k]\n",
    "    \n",
    "    base_model.load_state_dict(updated_model_state)\n",
    "    return base_model\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Choose GPU if available\n",
    "    print(f\"âœ… Using device: {device}\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Fix pad token to eos token\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\").to(device)\n",
    "\n",
    "    print(\"âœ… Base model loaded.\")\n",
    "\n",
    "    train_data, test_data = load_dataset()  # Load training and test data\n",
    "    print(\"âœ… Dataset loaded.\")\n",
    "\n",
    "    tokenized_train_data = tokenize_data(tokenizer, train_data, is_train=True, device=device)  # Tokenize the data\n",
    "    tokenized_test_data = tokenize_data(tokenizer, test_data, is_train=False, device=device)\n",
    "    print(\"âœ… Data tokenized.\")\n",
    "\n",
    "    adapter_name = \"alpaca_adapter\"\n",
    "    adapter_model = train_lora_adapter(base_model, tokenized_train_data, tokenized_test_data, adapter_name, device)\n",
    "    print(\"âœ… Adapter model trained.\")\n",
    "\n",
    "    task_vector = compute_task_vector(base_model, adapter_model, device)  # Compute task vector\n",
    "    gamma_value = compute_gamma_value(task_vector)  # Compute gamma value\n",
    "\n",
    "    final_model = apply_task_vector(base_model, task_vector, gamma_value, device)  # Apply task vector to base model\n",
    "    final_model.save_pretrained(\"./results/final_model\")  # Save the final model\n",
    "    print(\"âœ… Final model saved successfully.\")\n",
    "\n",
    "    return final_model\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    final_model = main()\n",
    "    print(\"ğŸ‰ Model training and adaptation completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc74f9d-84b6-4c30-9023-0a71067ac81b",
   "metadata": {},
   "source": [
    "To Check Weights Alpaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f100ca6-f74b-42f5-b6da-2e39a2af20a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored Weight Keys: ['base_model.model.model.layers.0.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.weight']\n",
      "Layer: base_model.model.model.layers.0.self_attn.q_proj.lora_A.weight\n",
      "Weights:\n",
      "tensor([[-0.0753,  0.0485,  0.0069,  ..., -0.0032,  0.0129,  0.0070],\n",
      "        [ 0.0304, -0.0466, -0.0517,  ..., -0.0018,  0.0071, -0.0019],\n",
      "        [ 0.0500,  0.0073,  0.0062,  ...,  0.0378,  0.0553, -0.0243],\n",
      "        ...,\n",
      "        [-0.0086,  0.0296, -0.0306,  ..., -0.0551,  0.0147, -0.0154],\n",
      "        [ 0.0628, -0.0603, -0.0247,  ...,  0.0801,  0.0099, -0.0279],\n",
      "        [-0.0466,  0.0562,  0.0215,  ...,  0.0008,  0.0548,  0.0284]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from safetensors.torch import load_file\n",
    "\n",
    "# Path to the LoRA adapter weights\n",
    "adapter_weights_path = \"/workspace/LLaMa-2-7B/results/alpaca_adapter/adapter_model.safetensors\"\n",
    "\n",
    "# Load the weights\n",
    "weights = load_file(adapter_weights_path)\n",
    "\n",
    "# List all weight tensors\n",
    "print(\"Stored Weight Keys:\", list(weights.keys()))\n",
    "\n",
    "# Example: Print the first few values of a weight tensor\n",
    "for key in weights:\n",
    "    print(f\"Layer: {key}\\nWeights:\\n{weights[key]}\\n\")\n",
    "    break  # Remove this break to print all weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3c7bfc-5268-4eb6-9723-a0f99929f2aa",
   "metadata": {},
   "source": [
    "Gamma Alpaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3cc43f0-574f-48d5-864a-688738442693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma: 32\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/workspace/LLaMa-2-7B/results/alpaca_adapter/adapter_config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(\"Gamma:\", config.get(\"lora_alpha\", \"Not found\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c33ad2b-4921-4cf8-93d0-ce80f3b2c45e",
   "metadata": {},
   "source": [
    "Base Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88930725-e4ab-4bd5-9478-fd7168b94bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec6609b82b74b668760c454b0ddcd10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Base model weights saved to base_model_weights.pth.\n",
      "âœ… Base model weights loaded from base_model_weights.pth.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "def save_base_model_weights(base_model, filename=\"base_model_weights.pth\"):\n",
    "    \"\"\"\n",
    "    Save the base model weights to a file.\n",
    "    Args:\n",
    "        base_model: The base model from which to save weights.\n",
    "        filename: The path where to save the weights.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        torch.save(base_model.state_dict(), filename)\n",
    "        print(f\"âœ… Base model weights saved to {filename}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ğŸš¨ Error saving base model weights: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def load_base_model_weights(base_model, filename=\"base_model_weights.pth\"):\n",
    "    \"\"\"\n",
    "    Load the base model weights from a file.\n",
    "    Args:\n",
    "        base_model: The base model to load weights into.\n",
    "        filename: The path from which to load the weights.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        base_model.load_state_dict(torch.load(filename))\n",
    "        print(f\"âœ… Base model weights loaded from {filename}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ğŸš¨ Error loading base model weights: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load your base model\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "    # Save the base model weights\n",
    "    save_base_model_weights(base_model)\n",
    "\n",
    "    # Optionally, load them back to verify\n",
    "    load_base_model_weights(base_model)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a642df5-8d56-404c-953b-b04d676f83cc",
   "metadata": {},
   "source": [
    "To Check Base Weights Alpaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a6f23a7-195f-4e20-9097-f5b4d73242ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight : torch.Size([32000, 4096])\n",
      "model.layers.0.self_attn.q_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.0.self_attn.k_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.0.self_attn.v_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.0.self_attn.o_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.0.mlp.gate_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.0.mlp.up_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.0.mlp.down_proj.weight : torch.Size([4096, 11008])\n",
      "model.layers.0.input_layernorm.weight : torch.Size([4096])\n",
      "model.layers.0.post_attention_layernorm.weight : torch.Size([4096])\n",
      "model.layers.1.self_attn.q_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.1.self_attn.k_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.1.self_attn.v_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.1.self_attn.o_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.1.mlp.gate_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.1.mlp.up_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.1.mlp.down_proj.weight : torch.Size([4096, 11008])\n",
      "model.layers.1.input_layernorm.weight : torch.Size([4096])\n",
      "model.layers.1.post_attention_layernorm.weight : torch.Size([4096])\n",
      "model.layers.2.self_attn.q_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.2.self_attn.k_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.2.self_attn.v_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.2.self_attn.o_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.2.mlp.gate_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.2.mlp.up_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.2.mlp.down_proj.weight : torch.Size([4096, 11008])\n",
      "model.layers.2.input_layernorm.weight : torch.Size([4096])\n",
      "model.layers.2.post_attention_layernorm.weight : torch.Size([4096])\n",
      "model.layers.3.self_attn.q_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.3.self_attn.k_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.3.self_attn.v_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.3.self_attn.o_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.3.mlp.gate_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.3.mlp.up_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.3.mlp.down_proj.weight : torch.Size([4096, 11008])\n",
      "model.layers.3.input_layernorm.weight : torch.Size([4096])\n",
      "model.layers.3.post_attention_layernorm.weight : torch.Size([4096])\n",
      "model.layers.4.self_attn.q_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.4.self_attn.k_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.4.self_attn.v_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.4.self_attn.o_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.4.mlp.gate_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.4.mlp.up_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.4.mlp.down_proj.weight : torch.Size([4096, 11008])\n",
      "model.layers.4.input_layernorm.weight : torch.Size([4096])\n",
      "model.layers.4.post_attention_layernorm.weight : torch.Size([4096])\n",
      "model.layers.5.self_attn.q_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.5.self_attn.k_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.5.self_attn.v_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.5.self_attn.o_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.5.mlp.gate_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.5.mlp.up_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.5.mlp.down_proj.weight : torch.Size([4096, 11008])\n",
      "model.layers.5.input_layernorm.weight : torch.Size([4096])\n",
      "model.layers.5.post_attention_layernorm.weight : torch.Size([4096])\n",
      "model.layers.6.self_attn.q_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.6.self_attn.k_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.6.self_attn.v_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.6.self_attn.o_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.6.mlp.gate_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.6.mlp.up_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.6.mlp.down_proj.weight : torch.Size([4096, 11008])\n",
      "model.layers.6.input_layernorm.weight : torch.Size([4096])\n",
      "model.layers.6.post_attention_layernorm.weight : torch.Size([4096])\n",
      "model.layers.7.self_attn.q_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.7.self_attn.k_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.7.self_attn.v_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.7.self_attn.o_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.7.mlp.gate_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.7.mlp.up_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.7.mlp.down_proj.weight : torch.Size([4096, 11008])\n",
      "model.layers.7.input_layernorm.weight : torch.Size([4096])\n",
      "model.layers.7.post_attention_layernorm.weight : torch.Size([4096])\n",
      "model.layers.8.self_attn.q_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.8.self_attn.k_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.8.self_attn.v_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.8.self_attn.o_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.8.mlp.gate_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.8.mlp.up_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.8.mlp.down_proj.weight : torch.Size([4096, 11008])\n",
      "model.layers.8.input_layernorm.weight : torch.Size([4096])\n",
      "model.layers.8.post_attention_layernorm.weight : torch.Size([4096])\n",
      "model.layers.9.self_attn.q_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.9.self_attn.k_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.9.self_attn.v_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.9.self_attn.o_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.9.mlp.gate_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.9.mlp.up_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.9.mlp.down_proj.weight : torch.Size([4096, 11008])\n",
      "model.layers.9.input_layernorm.weight : torch.Size([4096])\n",
      "model.layers.9.post_attention_layernorm.weight : torch.Size([4096])\n",
      "model.layers.10.self_attn.q_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.10.self_attn.k_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.10.self_attn.v_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.10.self_attn.o_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.10.mlp.gate_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.10.mlp.up_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.10.mlp.down_proj.weight : torch.Size([4096, 11008])\n",
      "model.layers.10.input_layernorm.weight : torch.Size([4096])\n",
      "model.layers.10.post_attention_layernorm.weight : torch.Size([4096])\n",
      "model.layers.11.self_attn.q_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.11.self_attn.k_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.11.self_attn.v_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.11.self_attn.o_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.11.mlp.gate_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.11.mlp.up_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.11.mlp.down_proj.weight : torch.Size([4096, 11008])\n",
      "model.layers.11.input_layernorm.weight : torch.Size([4096])\n",
      "model.layers.11.post_attention_layernorm.weight : torch.Size([4096])\n",
      "model.layers.12.self_attn.q_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.12.self_attn.k_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.12.self_attn.v_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.12.self_attn.o_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.12.mlp.gate_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.12.mlp.up_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.12.mlp.down_proj.weight : torch.Size([4096, 11008])\n",
      "model.layers.12.input_layernorm.weight : torch.Size([4096])\n",
      "model.layers.12.post_attention_layernorm.weight : torch.Size([4096])\n",
      "model.layers.13.self_attn.q_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.13.self_attn.k_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.13.self_attn.v_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.13.self_attn.o_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.13.mlp.gate_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.13.mlp.up_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.13.mlp.down_proj.weight : torch.Size([4096, 11008])\n",
      "model.layers.13.input_layernorm.weight : torch.Size([4096])\n",
      "model.layers.13.post_attention_layernorm.weight : torch.Size([4096])\n",
      "model.layers.14.self_attn.q_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.14.self_attn.k_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.14.self_attn.v_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.14.self_attn.o_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.14.mlp.gate_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.14.mlp.up_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.14.mlp.down_proj.weight : torch.Size([4096, 11008])\n",
      "model.layers.14.input_layernorm.weight : torch.Size([4096])\n",
      "model.layers.14.post_attention_layernorm.weight : torch.Size([4096])\n",
      "model.layers.15.self_attn.q_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.15.self_attn.k_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.15.self_attn.v_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.15.self_attn.o_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.15.mlp.gate_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.15.mlp.up_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.15.mlp.down_proj.weight : torch.Size([4096, 11008])\n",
      "model.layers.15.input_layernorm.weight : torch.Size([4096])\n",
      "model.layers.15.post_attention_layernorm.weight : torch.Size([4096])\n",
      "model.layers.16.self_attn.q_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.16.self_attn.k_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.16.self_attn.v_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.16.self_attn.o_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.16.mlp.gate_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.16.mlp.up_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.16.mlp.down_proj.weight : torch.Size([4096, 11008])\n",
      "model.layers.16.input_layernorm.weight : torch.Size([4096])\n",
      "model.layers.16.post_attention_layernorm.weight : torch.Size([4096])\n",
      "model.layers.17.self_attn.q_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.17.self_attn.k_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.17.self_attn.v_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.17.self_attn.o_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.17.mlp.gate_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.17.mlp.up_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.17.mlp.down_proj.weight : torch.Size([4096, 11008])\n",
      "model.layers.17.input_layernorm.weight : torch.Size([4096])\n",
      "model.layers.17.post_attention_layernorm.weight : torch.Size([4096])\n",
      "model.layers.18.self_attn.q_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.18.self_attn.k_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.18.self_attn.v_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.18.self_attn.o_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.18.mlp.gate_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.18.mlp.up_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.18.mlp.down_proj.weight : torch.Size([4096, 11008])\n",
      "model.layers.18.input_layernorm.weight : torch.Size([4096])\n",
      "model.layers.18.post_attention_layernorm.weight : torch.Size([4096])\n",
      "model.layers.19.self_attn.q_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.19.self_attn.k_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.19.self_attn.v_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.19.self_attn.o_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.19.mlp.gate_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.19.mlp.up_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.19.mlp.down_proj.weight : torch.Size([4096, 11008])\n",
      "model.layers.19.input_layernorm.weight : torch.Size([4096])\n",
      "model.layers.19.post_attention_layernorm.weight : torch.Size([4096])\n",
      "model.layers.20.self_attn.q_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.20.self_attn.k_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.20.self_attn.v_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.20.self_attn.o_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.20.mlp.gate_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.20.mlp.up_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.20.mlp.down_proj.weight : torch.Size([4096, 11008])\n",
      "model.layers.20.input_layernorm.weight : torch.Size([4096])\n",
      "model.layers.20.post_attention_layernorm.weight : torch.Size([4096])\n",
      "model.layers.21.self_attn.q_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.21.self_attn.k_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.21.self_attn.v_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.21.self_attn.o_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.21.mlp.gate_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.21.mlp.up_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.21.mlp.down_proj.weight : torch.Size([4096, 11008])\n",
      "model.layers.21.input_layernorm.weight : torch.Size([4096])\n",
      "model.layers.21.post_attention_layernorm.weight : torch.Size([4096])\n",
      "model.layers.22.self_attn.q_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.22.self_attn.k_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.22.self_attn.v_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.22.self_attn.o_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.22.mlp.gate_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.22.mlp.up_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.22.mlp.down_proj.weight : torch.Size([4096, 11008])\n",
      "model.layers.22.input_layernorm.weight : torch.Size([4096])\n",
      "model.layers.22.post_attention_layernorm.weight : torch.Size([4096])\n",
      "model.layers.23.self_attn.q_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.23.self_attn.k_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.23.self_attn.v_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.23.self_attn.o_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.23.mlp.gate_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.23.mlp.up_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.23.mlp.down_proj.weight : torch.Size([4096, 11008])\n",
      "model.layers.23.input_layernorm.weight : torch.Size([4096])\n",
      "model.layers.23.post_attention_layernorm.weight : torch.Size([4096])\n",
      "model.layers.24.self_attn.q_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.24.self_attn.k_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.24.self_attn.v_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.24.self_attn.o_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.24.mlp.gate_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.24.mlp.up_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.24.mlp.down_proj.weight : torch.Size([4096, 11008])\n",
      "model.layers.24.input_layernorm.weight : torch.Size([4096])\n",
      "model.layers.24.post_attention_layernorm.weight : torch.Size([4096])\n",
      "model.layers.25.self_attn.q_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.25.self_attn.k_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.25.self_attn.v_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.25.self_attn.o_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.25.mlp.gate_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.25.mlp.up_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.25.mlp.down_proj.weight : torch.Size([4096, 11008])\n",
      "model.layers.25.input_layernorm.weight : torch.Size([4096])\n",
      "model.layers.25.post_attention_layernorm.weight : torch.Size([4096])\n",
      "model.layers.26.self_attn.q_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.26.self_attn.k_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.26.self_attn.v_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.26.self_attn.o_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.26.mlp.gate_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.26.mlp.up_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.26.mlp.down_proj.weight : torch.Size([4096, 11008])\n",
      "model.layers.26.input_layernorm.weight : torch.Size([4096])\n",
      "model.layers.26.post_attention_layernorm.weight : torch.Size([4096])\n",
      "model.layers.27.self_attn.q_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.27.self_attn.k_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.27.self_attn.v_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.27.self_attn.o_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.27.mlp.gate_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.27.mlp.up_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.27.mlp.down_proj.weight : torch.Size([4096, 11008])\n",
      "model.layers.27.input_layernorm.weight : torch.Size([4096])\n",
      "model.layers.27.post_attention_layernorm.weight : torch.Size([4096])\n",
      "model.layers.28.self_attn.q_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.28.self_attn.k_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.28.self_attn.v_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.28.self_attn.o_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.28.mlp.gate_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.28.mlp.up_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.28.mlp.down_proj.weight : torch.Size([4096, 11008])\n",
      "model.layers.28.input_layernorm.weight : torch.Size([4096])\n",
      "model.layers.28.post_attention_layernorm.weight : torch.Size([4096])\n",
      "model.layers.29.self_attn.q_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.29.self_attn.k_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.29.self_attn.v_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.29.self_attn.o_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.29.mlp.gate_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.29.mlp.up_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.29.mlp.down_proj.weight : torch.Size([4096, 11008])\n",
      "model.layers.29.input_layernorm.weight : torch.Size([4096])\n",
      "model.layers.29.post_attention_layernorm.weight : torch.Size([4096])\n",
      "model.layers.30.self_attn.q_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.30.self_attn.k_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.30.self_attn.v_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.30.self_attn.o_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.30.mlp.gate_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.30.mlp.up_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.30.mlp.down_proj.weight : torch.Size([4096, 11008])\n",
      "model.layers.30.input_layernorm.weight : torch.Size([4096])\n",
      "model.layers.30.post_attention_layernorm.weight : torch.Size([4096])\n",
      "model.layers.31.self_attn.q_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.31.self_attn.k_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.31.self_attn.v_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.31.self_attn.o_proj.weight : torch.Size([4096, 4096])\n",
      "model.layers.31.mlp.gate_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.31.mlp.up_proj.weight : torch.Size([11008, 4096])\n",
      "model.layers.31.mlp.down_proj.weight : torch.Size([4096, 11008])\n",
      "model.layers.31.input_layernorm.weight : torch.Size([4096])\n",
      "model.layers.31.post_attention_layernorm.weight : torch.Size([4096])\n",
      "model.norm.weight : torch.Size([4096])\n",
      "lm_head.weight : torch.Size([32000, 4096])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load the model's state_dict (weights)\n",
    "model = torch.load('/workspace/LLaMa-2-7B/base_model_weights.pth')\n",
    "\n",
    "# Print the state_dict of the model\n",
    "for name, param in model.items():\n",
    "    print(f'{name} : {param.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf78fce-0d93-4672-9e1f-db3a8c369002",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
